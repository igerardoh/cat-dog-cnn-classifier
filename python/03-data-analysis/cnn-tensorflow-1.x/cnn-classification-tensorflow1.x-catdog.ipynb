{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAT-DOG IMAGE CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Status: <span style=color:orange;>In progress</span></b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LOAD IMAGE FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "data_dir = '../../../data/'\n",
    "classes = os.listdir(data_dir)  # get classes from folder names\n",
    "\n",
    "# load feature data\n",
    "filename = '../../02-data-preprocessing/output/preprocessed_data/X.pckl'\n",
    "loader = open(filename, 'rb')\n",
    "X = pickle.load(loader)\n",
    "loader.close()\n",
    "\n",
    "# load target data\n",
    "filename = '../../02-data-preprocessing/output/preprocessed_data/y.pckl'\n",
    "loader = open(filename, 'rb')\n",
    "y = pickle.load(loader)\n",
    "loader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SPLIT THE FILE NAMES INTO TRAINING AND TESTING SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes:\n",
      "\n",
      "X_train : (7200, 32, 32, 3)\n",
      "y_train : (7200, 2)\n",
      "\n",
      "X_test : (1800, 32, 32, 3)\n",
      "y_test : (1800, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# set the seed for reproducibility\n",
    "np.random.seed(127)\n",
    "\n",
    "# split the dataset into 2 training and 2 testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "print('Data shapes:\\n')\n",
    "print('X_train : {}\\ny_train : {}\\n\\nX_test : {}\\ny_test : {}'.format(np.shape(X_train),\n",
    "                                                                      np.shape(y_train),\n",
    "                                                                      np.shape(X_test),\n",
    "                                                                      np.shape(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DEFINE NETWORK PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract images properties\n",
    "image_width  = np.shape(X_test)[1]\n",
    "image_height = np.shape(X_test)[2]\n",
    "n_channel  = np.shape(X_test)[3]\n",
    "n_classes = y_train.shape[1]\n",
    "n_features = 3  # dummy\n",
    "\n",
    "# count number of samples in each set of data\n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# define amount of neurons\n",
    "n_layer_in = n_features  # 11 neurons in input layer\n",
    "n_layer_h1 = 50          # first   hidden layer\n",
    "n_layer_h2 = 50          # second  hidden layer\n",
    "n_layer_out = n_classes  # 7 neurons in input layer\n",
    "\n",
    "sigma_init = 0.01   # For randomized initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RESET TENSORFLOW GRAPH IF THERE IS ANY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# this will set up a specific seed in order to control the output \n",
    "# and get more homogeneous results though every model variation\n",
    "def reset_graph(seed=127):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MODEL ARCHITECTURE\n",
    "You can find useful information in the following link:\n",
    "https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional output size:\n",
    "$$\n",
    "Conv = \\frac{W-F+2P}{S}+1\n",
    "$$\n",
    "\n",
    "Maxpool downsampling :\n",
    "$$\n",
    "MaxPool = \\frac{W-F}{S}+1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up some parameters\n",
    "keep_prob = 0.75\n",
    "\n",
    "receptive_field = 3             # kernel filter = 3x3\n",
    "feature_map = 30\n",
    "\n",
    "# create symbolic variables\n",
    "X = tf.placeholder(tf.float32, [None, image_width, image_height, n_channel])\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# define some of the model's components\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "with tf.name_scope(\"cnn\"):\n",
    "    # Receptive Field : The range of the visual attention attention\n",
    "    # Feature Map : \n",
    "    \n",
    "    # ------ convolutional neural network ------\n",
    "    \n",
    "    W_conv1 = weight_variable([receptive_field, receptive_field, n_channel, feature_map])  # [3,3,3,32]\n",
    "    b_conv1 = bias_variable([feature_map])    # 32\n",
    "    h_conv1 = tf.nn.relu(conv2d(X, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    W_conv2 = weight_variable([receptive_field, receptive_field, feature_map, feature_map*2])  # [3,3,32,64]\n",
    "    b_conv2 = bias_variable([feature_map*2])   # 64\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "    \n",
    "    # ------ feed forward neural network ------\n",
    "    \n",
    "    W_fc1 = weight_variable([int(image_width/4 * image_height/4 * (feature_map*2)), feature_map**2]) # [65536,1024]\n",
    "    b_fc1 = bias_variable([feature_map**2])  # 1024\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, int(image_width/4 * image_height/4 * (feature_map*2))])  # [~,16384]\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    W_fc2 = weight_variable([feature_map**2, n_classes])\n",
    "    b_fc2 = bias_variable([n_classes])\n",
    "\n",
    "    y_pred = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up some parameters\n",
    "keep_prob = 0.75\n",
    "\n",
    "receptive_field = 3             # kernel filter = 3x3\n",
    "feature_map = 30\n",
    "\n",
    "# create symbolic variables\n",
    "X = tf.placeholder(tf.float32, [None, image_width, image_height, n_channel])\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# define some of the model's components\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "with tf.name_scope(\"cnn\"):\n",
    "    # Receptive Field : The range of the visual attention attention\n",
    "    # Feature Map : \n",
    "    \n",
    "    # ------ convolutional neural network ------\n",
    "    \n",
    "    W_conv1 = weight_variable([receptive_field, receptive_field, n_channel, feature_map])  # [3,3,3,32]\n",
    "    b_conv1 = bias_variable([feature_map])    # 32\n",
    "    h_conv1 = tf.nn.relu(conv2d(X, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    W_conv2 = weight_variable([receptive_field, receptive_field, feature_map, feature_map*2])  # [3,3,32,64]\n",
    "    b_conv2 = bias_variable([feature_map*2])   # 64\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "    \n",
    "    # ------ feed forward neural network ------\n",
    "    \n",
    "    W_fc1 = weight_variable([int(image_width/4 * image_height/4 * (feature_map*2)), feature_map**2]) # [65536,1024]\n",
    "    b_fc1 = bias_variable([feature_map**2])  # 1024\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, int(image_width/4 * image_height/4 * (feature_map*2))])  # [~,16384]\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    W_fc2 = weight_variable([feature_map**2, n_classes])\n",
    "    b_fc2 = bias_variable([n_classes])\n",
    "\n",
    "    y_pred = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LEARNING RATE CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying Learning Rate :  none\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.10\n",
    "\n",
    "# CHOOSE A DECAYING METHOD IN HERE\n",
    "model_decay = 'none'      # [exponential | inverse_time | natural_exponential | polynomial | none]\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "decay_rate = 0.10\n",
    "decay_step = 10000\n",
    "\n",
    "if model_decay == 'exponential':\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate, global_step, decay_step, decay_rate)\n",
    "\n",
    "elif model_decay == 'inverse_time':\n",
    "    learning_rate = tf.train.inverse_time_decay(learning_rate, global_step, decay_step, decay_rate)\n",
    "    \n",
    "elif model_decay == 'natural_exponential':\n",
    "    learning_rate = tf.train.natural_exp_decay(learning_rate, global_step, decay_step, decay_rate)\n",
    "    \n",
    "elif model_decay == 'polynomial':\n",
    "    end_learning_rate = 0.001\n",
    "    learning_rate = tf.train.polynomial_decay(learning_rate, global_step, decay_step, end_learning_rate, power=0.5)\n",
    "    \n",
    "else:\n",
    "    decay_rate = 1.0\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate, global_step, decay_step, decay_rate)\n",
    "\n",
    "print('Decaying Learning Rate : ', model_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OPTIMIZER AND ACCURACY CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    #loss = tf.square(Y - y_pred)                                                       # squared error\n",
    "    #loss = tf.nn.softmax(logits=y_pred)                                                # softmax\n",
    "    #loss = tf.nn.log_softmax(logits=y_pred)                                            # log-softmax\n",
    "    #loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=y_pred, dim=-1) # cross-entropy\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=y_pred)            # sigmoid-cross-entropy\n",
    "    #loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y, logits=y_pred)     # sparse-softmax-cross-entropy\n",
    "    loss = tf.reduce_mean(loss, name='MSE')\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate)                     # SGD\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9) # MOMENTUM\n",
    "    #optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)               # ADAGRAD\n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate)              # ADADELTA\n",
    "    #optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay=1)      # RMS\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "\n",
    "# Create summaries\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "tf.summary.scalar(\"learn_rate\", learning_rate)\n",
    "\n",
    "# Merge all summaries into a single op to generate the summary data\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DEFINE DIRECTORIES FOR RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# set up the directory to store the results for tensorboard\n",
    "now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "root_ckpoint = 'output/tf_checkpoints'\n",
    "root_logdir = 'output/tf_logs'\n",
    "logdir = '{}/run-{}/'.format(root_logdir, now) \n",
    "\n",
    "## Try to remove tree; if failed show an error using try...except on screen\n",
    "try:\n",
    "    shutil.rmtree(root_ckpoint)\n",
    "except OSError as e:\n",
    "    print (\"Error: %s - %s.\" % (e.filename, e.strerror))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MODEL EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy a progress bar\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    def tqdm(x, *args, **kwargs):\n",
    "        return x\n",
    "\n",
    "# define some parameters\n",
    "n_epochs = 150\n",
    "display_epoch = 2\n",
    "batch_size = 5\n",
    "n_batches = int(n_train/batch_size)\n",
    "\n",
    "# this will later help me to restore the model to a specific epoch\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "# store results through every epoch iteration\n",
    "acc_train_list = []\n",
    "acc_test_list = []\n",
    "avg_loss_list = []\n",
    "learning_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5511229550e4582817612426dbd57a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0002\tTrainAcc: 0.50722\tTestAcc: 0.52944\tLoss: 0.74759\tLearning: 0.1000000\n",
      "Epoch: 0004\tTrainAcc: 0.50528\tTestAcc: 0.51778\tLoss: 0.74899\tLearning: 0.1000000\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-0d87f8e788fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;31m# start model training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;31m# Run optimization (backprop), cost and summary nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gerardo_paniagua\\.conda\\envs\\vision\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gerardo_paniagua\\.conda\\envs\\vision\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gerardo_paniagua\\.conda\\envs\\vision\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gerardo_paniagua\\.conda\\envs\\vision\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gerardo_paniagua\\.conda\\envs\\vision\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gerardo_paniagua\\.conda\\envs\\vision\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # write logs for tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logdir, graph=tf.get_default_graph())\n",
    "    \n",
    "    for epoch in tqdm(range(1, n_epochs+1)):\n",
    "        avg_loss = 0    # accumulate batch average loss for each epoch\n",
    "        \n",
    "        for i in range(0, n_train, batch_size):\n",
    "            # create batches\n",
    "            X_batch = X_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "            \n",
    "            # start model training\n",
    "            sess.run(training_op, feed_dict={X:X_batch, Y:y_batch})\n",
    "            \n",
    "            # Run optimization (backprop), cost and summary nodes\n",
    "            _, _loss, _summary = sess.run([training_op, loss, merged_summary_op],\n",
    "                                          feed_dict={X:X_batch, Y:y_batch})\n",
    "            avg_loss += _loss/n_batches\n",
    "            \n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(_summary)\n",
    "            \n",
    "        # after the epoch is finished this will append the loss\n",
    "        avg_loss_list.append(avg_loss)\n",
    "            \n",
    "        # measure performance and display the results\n",
    "        if (epoch+1) % display_epoch == 0:\n",
    "            _acc_train = accuracy.eval(feed_dict={X: X_train, Y:y_train})\n",
    "            _acc_test = accuracy.eval(feed_dict={X: X_test, Y: y_test})\n",
    "            \n",
    "            # append results to lists\n",
    "            acc_train_list.append(_acc_train); acc_test_list.append(_acc_test)\n",
    "            learning_list.append(sess.run(learning_rate))\n",
    "\n",
    "            # Save model weights to disk for reproducibility\n",
    "            saver = tf.train.Saver(max_to_keep=15)\n",
    "            saver.save(sess, \"{}/epoch{:04}.ckpt\".format(root_ckpoint, (epoch+1)))\n",
    "            \n",
    "            print(\"Epoch: {:04}\\tTrainAcc: {:06.5f}\\tTestAcc: {:06.5f}\\tLoss: {:06.5f}\\tLearning: {:06.7f}\".format((epoch+1),\n",
    "                                                                                                                   _acc_train,\n",
    "                                                                                                                   _acc_test,\n",
    "                                                                                                                   avg_loss,\n",
    "                                                                                                                   sess.run(learning_rate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VISUALIZE THE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# set up legend\n",
    "blue_patch = mpatches.Patch(color='blue', label='Train Accuracy [Maximize]')\n",
    "red_patch = mpatches.Patch(color='red', label='Test Accuracy [Maximize]')\n",
    "plt.legend(handles=[blue_patch,red_patch])\n",
    "\n",
    "#plot the data\n",
    "plt.plot(acc_train_list, color='blue')\n",
    "plt.plot(acc_test_list, color='red')\n",
    "\n",
    "plt.xlabel('epochs (x{})'.format(display_epoch))\n",
    "plt.ylabel('score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LEARNING RATE EVOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_patch = mpatches.Patch(color='orange', label='Learning rate')\n",
    "plt.legend(handles=[or_patch])\n",
    "\n",
    "plt.plot(learning_list, color='orange');\n",
    "plt.xlabel('epochs (x{})'.format(display_epoch))\n",
    "plt.ylabel('learning rate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
